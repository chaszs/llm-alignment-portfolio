# LLM Evaluation, Alignment and Language Analysis Portfolio

This repository documents my practical experience in the evaluation, alignment, and qualitative analysis of large language models (LLMs) and speech systems.

The work presented here focuses on applied evaluation rather than model training, covering tasks related to alignment, reasoning quality, linguistic robustness, and real-world language behavior.

All materials are synthetic and designed exclusively for professional portfolio purposes.

---

## Repository Structure

The repository is organized into independent but related modules, each addressing a specific aspect of model evaluation and language processing.

projects/
‚îú‚îÄ 01-judge/
‚îú‚îÄ 02-idioms/
‚îî‚îÄ 03-speech/


Each module includes its own documentation, scope definition, evaluation criteria, and illustrative synthetic examples where applicable.

---

## Modules Overview

### 1. Judge ‚Äì LLM Evaluation and Alignment

The **Judge** module documents work performed under a structured *LLM-as-a-Judge* evaluation framework.

It focuses on the rigorous application of binary and Likert-scale rubrics to assess:

- Instruction following
- Factual accuracy
- Reasoning quality
- Semantic fidelity
- Linguistic quality
- Safety and boundary handling

The module covers a defined subset of Judge tasks and demonstrates methodological rigor in model evaluation and alignment.

üìÅ `projects/01-judge/`

---

### 2. Idioms ‚Äì Non-Literal Language and Semantic Robustness

The **Idioms** module explores how language models handle non-literal expressions, idiomatic constructions, and meaning that cannot be resolved compositionally.

The focus is qualitative and linguistic, addressing:

- Idiomatic meaning interpretation
- Semantic ambiguity
- Paraphrasing robustness
- Cross-linguistic variation

The module reflects evaluation work conducted in multilingual contexts (Spanish, Catalan, and English), with an emphasis on semantic fidelity rather than scoring.

üìÅ `projects/02-idioms/`

---

### 3. Speech ‚Äì ASR and Spoken Language Evaluation

The **Speech** module addresses the evaluation of automatic speech recognition (ASR) systems and speech pipelines under real-world conditions.

Rather than benchmark-driven performance, the module focuses on:

- Robustness to noise and signal degradation
- Handling of spontaneous and non-standard speech
- Accent and phonetic variation
- Error patterns and hallucination risks
- Recognition of system limits

The emphasis is on applied evaluation at the boundary between acoustic signal and language.

üìÅ `projects/03-speech/`

---

## Methodological Approach

Across all modules, the evaluation approach is characterized by:

- Clear scoping and task delimitation
- Context-aware qualitative analysis
- Explicit criteria and evaluation principles
- Separation between evaluation and model training
- Awareness of alignment, safety, and user impact

The repository is intended to demonstrate analytical rigor, linguistic sensitivity, and practical experience with real-world model evaluation workflows.

---

## Disclaimer

All examples, datasets, and outputs contained in this repository are synthetic.

No proprietary data, confidential materials, or internal systems are reproduced or disclosed.




